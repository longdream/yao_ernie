"""
ACEä»»åŠ¡åŒ¹é…å™¨
è®¡ç®—ä»»åŠ¡ç›¸ä¼¼åº¦å¹¶å¤ç”¨å†å²ä»»åŠ¡çš„plan JSON
"""
import json
import re
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from collections import Counter
import difflib

from planscope.core.exceptions import TaskMatchingError


class TaskMatcher:
    """
    ä»»åŠ¡åŒ¹é…å™¨
    
    è´Ÿè´£è®¡ç®—ä»»åŠ¡ç›¸ä¼¼åº¦ï¼ŒæŸ¥æ‰¾å¯å¤ç”¨çš„å†å²ä»»åŠ¡
    """
    
    def __init__(self, work_dir: str, logger_manager, llm_analyzer=None, storage_manager=None, vector_db_manager=None):
        """
        åˆå§‹åŒ–ä»»åŠ¡åŒ¹é…å™¨
        
        Args:
            work_dir: å·¥ä½œç›®å½•
            logger_manager: æ—¥å¿—ç®¡ç†å™¨
            llm_analyzer: LLMåˆ†æå™¨ï¼ˆå¯é€‰ï¼Œç”¨äºæ™ºèƒ½åˆ¤æ–­ï¼‰
            storage_manager: å­˜å‚¨ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ï¼‰
            vector_db_manager: å‘é‡æ•°æ®åº“ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼Œç”¨äºé«˜æ•ˆæ£€ç´¢ï¼‰
        """
        self.work_dir = Path(work_dir)
        self.logger_manager = logger_manager
        self.logger = logger_manager.get_logger("task_matcher")
        self.storage_manager = storage_manager
        
        # LLMåˆ†æå™¨
        self.llm_analyzer = llm_analyzer
        
        # å‘é‡æ•°æ®åº“ç®¡ç†å™¨
        self.vector_db = vector_db_manager
        
        # ä»»åŠ¡å†å²ç›®å½•
        if storage_manager:
            self.task_history_dir = storage_manager.get_path("tasks")
        else:
            self.task_history_dir = self.work_dir / "task_history"
            self.task_history_dir.mkdir(parents=True, exist_ok=True)
    
    async def find_similar_tasks(self,
                          task_description: str,
                          threshold: float = 0.8,
                          max_candidates: int = 20) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼ä»»åŠ¡ï¼ˆä½¿ç”¨å‘é‡æ•°æ®åº“ï¼‰
        
        é€šè¿‡å‘é‡è¿‘ä¼¼æœç´¢æ‰¾åˆ°ç›¸ä¼¼ä»»åŠ¡IDï¼Œç„¶åä»JSONæ–‡ä»¶åŠ è½½è¯¦ç»†æ•°æ®
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
            max_candidates: æœ€å¤§å€™é€‰ä»»åŠ¡æ•°ï¼ˆé»˜è®¤20ï¼‰
            
        Returns:
            ç›¸ä¼¼ä»»åŠ¡åˆ—è¡¨ [(task_id, similarity, task_data), ...]
        """
        self.logger.info(f"æŸ¥æ‰¾ç›¸ä¼¼ä»»åŠ¡ï¼Œé˜ˆå€¼: {threshold}, Top-K: {max_candidates}")
        
        # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å¯ç”¨
        if not self.vector_db or not self.vector_db.is_available():
            self.logger.error("å‘é‡æ•°æ®åº“ä¸å¯ç”¨ï¼è¯·å…ˆè¿è¡Œ: python migrate_to_vector_db.py")
            return []
        
        # ä½¿ç”¨å‘é‡æ•°æ®åº“æ£€ç´¢
        return await self._find_with_vector_db(task_description, threshold, max_candidates)
    
    async def _find_with_vector_db(self,
                            task_description: str,
                            threshold: float,
                            max_candidates: int) -> List[Tuple[str, float, Dict[str, Any]]]:
        """
        ä½¿ç”¨å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸ä¼¼ä»»åŠ¡
        
        æµç¨‹ï¼šå‘é‡è¿‘ä¼¼æœç´¢ â†’ è·å–ç›¸ä¼¼ä»»åŠ¡ID â†’ ä»JSONåŠ è½½è¯¦ç»†æ•°æ®
        
        Returns:
            ç›¸ä¼¼ä»»åŠ¡åˆ—è¡¨ [(task_id, similarity, task_data), ...]
        """
        self.logger.info("ğŸ” å‘é‡æ•°æ®åº“æ£€ç´¢ä¸­...")
        
        # å¿«é€Ÿæ£€æŸ¥ï¼šæ•°æ®åº“æ˜¯å¦ä¸ºç©º
        try:
            stats = self.vector_db.get_stats()
            if not stats.get('available', False):
                self.logger.warning("å‘é‡æ•°æ®åº“ä¸å¯ç”¨ï¼Œè·³è¿‡æ£€ç´¢")
                return []
            
            if stats.get('total_tasks', 0) == 0:
                self.logger.info("âœ“ å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œè·³è¿‡æ£€ç´¢")
                return []
        except Exception as e:
            self.logger.warning(f"å‘é‡æ•°æ®åº“æ£€æŸ¥å¤±è´¥: {str(e)}ï¼Œè·³è¿‡æ£€ç´¢")
            return []
        
        # 1. è®¡ç®—æŸ¥è¯¢embeddingï¼ˆç›´æ¥awaitï¼Œä¸ä½¿ç”¨run_coroutine_threadsafeï¼‰
        self.logger.info("ğŸ“Š è®¡ç®—æŸ¥è¯¢embedding...")
        query_embedding = await self.llm_analyzer._get_embedding(task_description)
        self.logger.info(f"âœ“ Embeddingè®¡ç®—å®Œæˆï¼Œç»´åº¦: {len(query_embedding)}")
        
        # 2. å‘é‡è¿‘ä¼¼æœç´¢ï¼ˆå¿«é€Ÿæ‰¾åˆ°å€™é€‰IDï¼‰
        self.logger.info("ğŸ” æ‰§è¡Œå‘é‡æœç´¢...")
        results = await self.vector_db.search_similar_tasks(query_embedding, top_k=max_candidates)
        self.logger.info(f"âœ“ å‘é‡æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªå€™é€‰")
        
        # 3. è¿‡æ»¤é˜ˆå€¼ + ä»JSONåŠ è½½è¯¦ç»†æ•°æ®
        similar_tasks = []
        for result in results:
            if result['similarity'] >= threshold:
                flow_id = result['flow_id']
                task_id = result['metadata'].get('task_id', f"task_{flow_id}")
                
                # å‘é‡åº“åªå­˜IDå’Œmetadataï¼ŒçœŸå®æ•°æ®åœ¨JSONä¸­
                task_data = self._load_task_json(flow_id)
                if task_data:
                    similar_tasks.append((task_id, result['similarity'], task_data))
                else:
                    self.logger.warning(f"å‘é‡åº“ä¸­æœ‰è®°å½•ä½†JSONæ–‡ä»¶ç¼ºå¤±: {flow_id}")
        
        self.logger.info(f"âœ“ å‘é‡æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(similar_tasks)} ä¸ªç›¸ä¼¼ä»»åŠ¡ï¼ˆé˜ˆå€¼â‰¥{threshold}ï¼‰")
        return similar_tasks
    
    
    def _load_task_json(self, flow_id: str) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½ä»»åŠ¡JSONæ•°æ®
        
        Args:
            flow_id: Flow ID
            
        Returns:
            ä»»åŠ¡æ•°æ®ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å›None
        """
        task_file = self.task_history_dir / f"task_{flow_id}.json"
        
        if not task_file.exists():
            self.logger.warning(f"ä»»åŠ¡æ–‡ä»¶ä¸å­˜åœ¨: {task_file}")
            return None
        
        try:
            with open(task_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.error(f"åŠ è½½ä»»åŠ¡æ–‡ä»¶å¤±è´¥ ({task_file}): {str(e)}")
            return None
    
    def calculate_task_similarity(self, task1: str, task2: str) -> float:
        """
        ä½¿ç”¨embeddingè®¡ç®—ä»»åŠ¡ç›¸ä¼¼åº¦ï¼ˆå¢å¼ºæ—¥å¿—ï¼‰
        
        Args:
            task1: ä»»åŠ¡æè¿°1
            task2: ä»»åŠ¡æè¿°2
            
        Returns:
            ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
        """
        if not self.llm_analyzer:
            similarity = difflib.SequenceMatcher(None, task1.lower(), task2.lower()).ratio()
            self.logger.debug(f"ä½¿ç”¨æ–‡æœ¬åŒ¹é…: {similarity:.3f}")
            return similarity
        
        try:
            self.logger.debug(f"è®¡ç®—embeddingç›¸ä¼¼åº¦: '{task1[:50]}...' vs '{task2[:50]}...'")
            # ä½¿ç”¨embeddingè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
            similarity = self.llm_analyzer.calculate_embedding_similarity_sync(task1, task2)
            self.logger.info(f"Embeddingç›¸ä¼¼åº¦: {similarity:.3f}")
            return similarity
            
        except Exception as e:
            self.logger.error(f"embeddingç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}")
            raise  # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸ä½¿ç”¨é™çº§æ–¹æ¡ˆ
    
    def extract_keywords(self, text: str) -> List[str]:
        """
        æå–å…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸»è¦ç”¨äºå‘åå…¼å®¹ï¼‰
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # ç®€å•çš„åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²ï¼‰
        words = re.findall(r'\w+', text.lower())
        
        # è¿‡æ»¤çŸ­è¯
        keywords = [w for w in words if len(w) > 1]
        
        return keywords[:10]  # é™åˆ¶æ•°é‡
    
    def save_task_mapping(self,
                         task_description: str,
                         plan_json: Dict[str, Any],
                         success: bool) -> str:
        """
        ä¿å­˜ä»»åŠ¡æ˜ å°„ï¼ˆåŒæ—¶ä¿å­˜åˆ°JSONæ–‡ä»¶å’Œå‘é‡æ•°æ®åº“ï¼‰
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            plan_json: å·¥ä½œæµJSON
            success: æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
            
        Returns:
            ä»»åŠ¡ID
        """
        try:
            # ç”Ÿæˆä»»åŠ¡ID
            flow_id = plan_json.get("flow_id", "unknown")
            task_id = f"task_{flow_id}"
            
            # æ„å»ºä»»åŠ¡æ•°æ®
            task_data = {
                "task_id": task_id,
                "flow_id": flow_id,
                "task_description": task_description,
                "plan_json": plan_json,
                "success": success,
                "created_at": plan_json.get("created_at", ""),
                "keywords": self.extract_keywords(task_description)
            }
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            if self.storage_manager:
                self.storage_manager.save_task(flow_id, task_data)
            else:
                self.task_history_dir.mkdir(parents=True, exist_ok=True)
                task_file = self.task_history_dir / f"{task_id}.json"
                with open(task_file, 'w', encoding='utf-8') as f:
                    json.dump(task_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ“ ä»»åŠ¡æ˜ å°„å·²ä¿å­˜åˆ°JSON: {task_id}")
            
            # åŒæ­¥åˆ°å‘é‡æ•°æ®åº“
            if self.vector_db and self.vector_db.is_available():
                try:
                    import asyncio
                    
                    # å‡†å¤‡metadata
                    metadata = {
                        'task_id': task_id,
                        'success': success,
                        'created_at': plan_json.get("created_at", ""),
                        'app_name': plan_json.get("app_name", ""),
                        'steps_count': len(plan_json.get("steps", [])),
                        'complexity_level': plan_json.get("complexity_level", "")
                    }
                    
                    # å¼‚æ­¥æ·»åŠ åˆ°å‘é‡åº“
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        asyncio.create_task(
                            self.vector_db.add_task(flow_id, task_description, None, metadata)
                        )
                    else:
                        loop.run_until_complete(
                            self.vector_db.add_task(flow_id, task_description, None, metadata)
                        )
                    
                    self.logger.debug(f"âœ“ ä»»åŠ¡å·²åŒæ­¥åˆ°å‘é‡åº“: {task_id}")
                    
                except Exception as e:
                    self.logger.warning(f"åŒæ­¥åˆ°å‘é‡åº“å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {str(e)}")
            
            return task_id
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ä»»åŠ¡æ˜ å°„å¤±è´¥: {str(e)}")
            raise TaskMatchingError(f"ä¿å­˜ä»»åŠ¡æ˜ å°„å¤±è´¥: {str(e)}")
    
    def load_successful_plan(self, task_id: str) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½æˆåŠŸçš„plan
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            plan JSONï¼ˆå¦‚æœæ‰¾åˆ°ä¸”æˆåŠŸï¼‰
        """
        try:
            task_file = self.task_history_dir / f"{task_id}.json"
            
            if not task_file.exists():
                return None
            
            with open(task_file, 'r', encoding='utf-8') as f:
                task_data = json.load(f)
            
            # åªè¿”å›æˆåŠŸçš„plan
            if task_data.get("success", False):
                return task_data.get("plan_json")
            
            return None
            
        except Exception as e:
            self.logger.error(f"åŠ è½½planå¤±è´¥: {str(e)}")
            return None
    
    def get_task_history(self, limit: int = 20) -> List[Dict[str, Any]]:
        """
        è·å–ä»»åŠ¡å†å²
        
        Args:
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            ä»»åŠ¡å†å²åˆ—è¡¨
        """
        try:
            # è·å–æ‰€æœ‰ä»»åŠ¡æ–‡ä»¶ï¼ˆåŒ¹é…task_flow_*.jsonæ ¼å¼ï¼‰
            import os
            self.logger.info(f"[DEBUG] get_task_history called")
            self.logger.info(f"[DEBUG] Current working directory: {os.getcwd()}")
            self.logger.info(f"[DEBUG] task_history_dir (relative): {self.task_history_dir}")
            self.logger.info(f"[DEBUG] task_history_dir (absolute): {self.task_history_dir.absolute()}")
            self.logger.info(f"[DEBUG] task_history_dir exists: {self.task_history_dir.exists()}")
            self.logger.info(f"[DEBUG] Searching for pattern: task_flow_*.json")
            
            task_files = sorted(
                self.task_history_dir.glob("task_flow_*.json"),
                key=lambda f: f.stat().st_mtime,
                reverse=True
            )
            
            self.logger.info(f"[DEBUG] Found {len(task_files)} task files")
            if len(task_files) > 0:
                self.logger.info(f"[DEBUG] First 3 files: {[f.name for f in task_files[:3]]}")
            
            # åŠ è½½ä»»åŠ¡
            tasks = []
            for task_file in task_files[:limit]:
                try:
                    with open(task_file, 'r', encoding='utf-8') as f:
                        task_data = json.load(f)
                    
                    # æå–å…³é”®å­—æ®µåˆ°é¡¶å±‚ï¼Œæ–¹ä¾¿å‰ç«¯ä½¿ç”¨
                    if "plan_json" in task_data:
                        plan_json = task_data["plan_json"]
                        # æå–original_queryåˆ°é¡¶å±‚
                        if "original_query" in plan_json:
                            task_data["original_query"] = plan_json["original_query"]
                        # æå–stepsæ•°é‡
                        if "steps" in plan_json:
                            task_data["steps_count"] = len(plan_json["steps"])
                        # ä»task_descriptionä¸­æå–app_nameï¼ˆå¦‚æœå­˜åœ¨"ç›®æ ‡åº”ç”¨:"ï¼‰
                        if "task_description" in task_data:
                            desc = task_data["task_description"]
                            if "ç›®æ ‡åº”ç”¨:" in desc:
                                # æå–"ç›®æ ‡åº”ç”¨: XXX"ä¸­çš„XXX
                                import re
                                match = re.search(r'ç›®æ ‡åº”ç”¨:\s*([^\n(]+)', desc)
                                if match:
                                    task_data["app_name"] = match.group(1).strip()
                    
                    tasks.append(task_data)
                except Exception as e:
                    self.logger.warning(f"åŠ è½½ä»»åŠ¡æ–‡ä»¶å¤±è´¥ {task_file}: {str(e)}")
            
            self.logger.info(f"[DEBUG] Returning {len(tasks)} tasks")
            return tasks
            
        except Exception as e:
            self.logger.error(f"è·å–ä»»åŠ¡å†å²å¤±è´¥: {str(e)}")
            return []
    
    async def get_best_match(self, task_description: str, threshold: float = 0.8) -> Optional[Tuple[str, float, Dict[str, Any]]]:
        """
        è·å–æœ€ä½³åŒ¹é…ä»»åŠ¡
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            æœ€ä½³åŒ¹é… (task_id, similarity, task_data) æˆ– None
        """
        similar_tasks = await self.find_similar_tasks(task_description, threshold)
        
        if similar_tasks:
            # è¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„æˆåŠŸä»»åŠ¡
            for task_id, similarity, task_data in similar_tasks:
                if task_data.get("success", False):
                    return (task_id, similarity, task_data)
        
        return None
    
    def clear_history(self) -> int:
        """
        æ¸…ç†ä»»åŠ¡å†å²
        
        Returns:
            åˆ é™¤çš„æ–‡ä»¶æ•°é‡
        """
        count = 0
        for task_file in self.task_history_dir.glob("task_*.json"):
            try:
                task_file.unlink()
                count += 1
            except Exception as e:
                self.logger.warning(f"åˆ é™¤ä»»åŠ¡æ–‡ä»¶å¤±è´¥ {task_file}: {str(e)}")
        
        self.logger.info(f"æ¸…ç†äº† {count} ä¸ªä»»åŠ¡å†å²æ–‡ä»¶")
        return count
    
    def find_exact_match_plan(self, task_description: str) -> Optional[Dict[str, Any]]:
        """
        æŸ¥æ‰¾å®Œå…¨åŒ¹é…çš„ä»»åŠ¡planï¼Œç”¨äºå¿«é€Ÿå¤ç”¨
        ä½¿ç”¨normalize_task_descriptionç¡®ä¿åŒ¹é…å‡†ç¡®æ€§
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            
        Returns:
            åŒ¹é…çš„plan JSONï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        try:
            # æ ‡å‡†åŒ–ä»»åŠ¡æè¿°
            if self.storage_manager:
                normalized = self.storage_manager.normalize_task_description(task_description)
            else:
                # å‘åå…¼å®¹ï¼šç®€å•æ ‡å‡†åŒ–
                import re
                normalized = re.sub(r'\s+', ' ', task_description.strip()).lower()
            
            # éå†æ‰€æœ‰ä»»åŠ¡ï¼Œæ‰¾åˆ°æ ‡å‡†åŒ–åå®Œå…¨åŒ¹é…çš„
            for task_file in self.task_history_dir.glob("task_*.json"):
                try:
                    # ä»æ–‡ä»¶åæå–flow_id
                    flow_id = task_file.stem.replace("task_", "")
                    
                    # åŠ è½½ä»»åŠ¡æ•°æ®
                    if self.storage_manager:
                        task_data = self.storage_manager.load_task(flow_id)
                    else:
                        # å‘åå…¼å®¹ï¼šç›´æ¥è¯»å–
                        with open(task_file, 'r', encoding='utf-8') as f:
                            task_data = json.load(f)
                    
                    # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æˆåŠŸä¸”æœ‰plan
                    if task_data and task_data.get("success") and "plan_json" in task_data:
                        saved_desc = task_data.get("task_description", "")
                        
                        # æ ‡å‡†åŒ–ä¿å­˜çš„ä»»åŠ¡æè¿°
                        if self.storage_manager:
                            saved_normalized = self.storage_manager.normalize_task_description(saved_desc)
                        else:
                            import re
                            saved_normalized = re.sub(r'\s+', ' ', saved_desc.strip()).lower()
                        
                        # å®Œå…¨åŒ¹é…
                        if saved_normalized == normalized:
                            self.logger.info(f"æ‰¾åˆ°å®Œå…¨åŒ¹é…çš„ä»»åŠ¡: {flow_id}")
                            
                            # ä¼˜å…ˆä»plansç›®å½•è¯»å–æœ€æ–°çš„planï¼ˆç”¨æˆ·å¯èƒ½å·²ç¼–è¾‘ï¼‰
                            # å¦‚æœplansç›®å½•ä¸­æ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨task_historyä¸­çš„åŸå§‹plan
                            if self.storage_manager:
                                plans_dir = self.storage_manager.get_path("plans")
                                plan_file = plans_dir / f"{flow_id}.json"
                                if plan_file.exists():
                                    try:
                                        with open(plan_file, 'r', encoding='utf-8') as f:
                                            latest_plan = json.load(f)
                                        self.logger.info(f"âœ“ ä½¿ç”¨plansç›®å½•ä¸­çš„æœ€æ–°plan: {flow_id}")
                                        return latest_plan
                                    except Exception as e:
                                        self.logger.warning(f"è¯»å–plansç›®å½•ä¸­çš„planå¤±è´¥ï¼Œä½¿ç”¨task_historyä¸­çš„åŸå§‹plan: {e}")
                            
                            # Fallback: ä½¿ç”¨task_historyä¸­çš„åŸå§‹plan
                            return task_data.get("plan_json")
                
                except Exception as e:
                    self.logger.warning(f"è¯»å–ä»»åŠ¡æ–‡ä»¶å¤±è´¥ {task_file}: {str(e)}")
                    continue
            
            self.logger.debug(f"æœªæ‰¾åˆ°å®Œå…¨åŒ¹é…çš„ä»»åŠ¡")
            return None
            
        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾ç²¾ç¡®åŒ¹é…planå¤±è´¥: {str(e)}")
            return None

